{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a19f9837-13c2-46c0-a674-f3a156a7a66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from memory_profiler import profile\n",
    "\n",
    "from rl2023.constants import EX4_PENDULUM_CONSTANTS as PENDULUM_CONSTANTS\n",
    "from rl2023.constants import EX4_BIPEDAL_CONSTANTS as BIPEDAL_CONSTANTS\n",
    "from rl2023.exercise4.agents import DDPG\n",
    "from rl2023.exercise3.replay import ReplayBuffer\n",
    "from rl2023.util.hparam_sweeping import generate_hparam_configs\n",
    "from rl2023.util.result_processing import Run\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from typing import Dict, Iterable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from rl2023.exercise3.agents import Agent\n",
    "from rl2023.exercise3.networks import FCNetwork\n",
    "from rl2023.exercise3.replay import Transition\n",
    "\n",
    "class DiagGaussian(torch.nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def sample(self):\n",
    "        eps = Variable(torch.randn(*self.mean.size()))\n",
    "        return self.mean + self.std * eps\n",
    "\n",
    "\n",
    "class DDPG(Agent):\n",
    "    \"\"\" DDPG\n",
    "\n",
    "        ** YOU NEED TO IMPLEMENT THE FUNCTIONS IN THIS CLASS **\n",
    "\n",
    "        :attr critic (FCNetwork): fully connected critic network\n",
    "        :attr critic_optim (torch.optim): PyTorch optimiser for critic network\n",
    "        :attr policy (FCNetwork): fully connected actor network for policy\n",
    "        :attr policy_optim (torch.optim): PyTorch optimiser for actor network\n",
    "        :attr gamma (float): discount rate gamma\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            action_space: gym.Space,\n",
    "            observation_space: gym.Space,\n",
    "            gamma: float,\n",
    "            critic_learning_rate: float,\n",
    "            policy_learning_rate: float,\n",
    "            critic_hidden_size: Iterable[int],\n",
    "            policy_hidden_size: Iterable[int],\n",
    "            tau: float,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param action_space (gym.Space): environment's action space\n",
    "        :param observation_space (gym.Space): environment's observation space\n",
    "        :param gamma (float): discount rate gamma\n",
    "        :param critic_learning_rate (float): learning rate for critic optimisation\n",
    "        :param policy_learning_rate (float): learning rate for policy optimisation\n",
    "        :param critic_hidden_size (Iterable[int]): list of hidden dimensionalities for fully connected critic\n",
    "        :param policy_hidden_size (Iterable[int]): list of hidden dimensionalities for fully connected policy\n",
    "        :param tau (float): step for the update of the target networks\n",
    "        \"\"\"\n",
    "        super().__init__(action_space, observation_space)\n",
    "        STATE_SIZE = observation_space.shape[0]\n",
    "        ACTION_SIZE = action_space.shape[0]\n",
    "\n",
    "        self.upper_action_bound = action_space.high[0]\n",
    "        self.lower_action_bound = action_space.low[0]\n",
    "\n",
    "        # ######################################### #\n",
    "        #  BUILD YOUR NETWORKS AND OPTIMIZERS HERE  #\n",
    "        # ######################################### #\n",
    "        # self.actor = Actor(STATE_SIZE, policy_hidden_size, ACTION_SIZE)\n",
    "        self.actor = FCNetwork(#the guy generating actions\n",
    "            (STATE_SIZE, *policy_hidden_size, ACTION_SIZE), output_activation=torch.nn.Tanh\n",
    "        )\n",
    "        self.actor_target = FCNetwork(\n",
    "            (STATE_SIZE, *policy_hidden_size, ACTION_SIZE), output_activation=torch.nn.Tanh\n",
    "        )\n",
    "\n",
    "        self.actor_target.hard_update(self.actor)\n",
    "        # self.critic = Critic(STATE_SIZE + ACTION_SIZE, critic_hidden_size)\n",
    "        # self.critic_target = Critic(STATE_SIZE + ACTION_SIZE, critic_hidden_size)\n",
    "\n",
    "        self.critic = FCNetwork(\n",
    "            (STATE_SIZE + ACTION_SIZE, *critic_hidden_size, 1), output_activation=None\n",
    "        )\n",
    "        self.critic_target = FCNetwork(\n",
    "            (STATE_SIZE + ACTION_SIZE, *critic_hidden_size, 1), output_activation=None\n",
    "        )\n",
    "        self.critic_target.hard_update(self.critic)\n",
    "\n",
    "        self.policy_optim = Adam(self.actor.parameters(), lr=policy_learning_rate, eps=1e-3)\n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=critic_learning_rate, eps=1e-3)\n",
    "\n",
    "        for parameter in self.critic_target.parameters():\n",
    "            parameter.requires_grad=False\n",
    "        for parameter in self.actor_target.parameters():\n",
    "            parameter.requires_grad=False\n",
    "\n",
    "        # ############################################# #\n",
    "        # WRITE ANY HYPERPARAMETERS YOU MIGHT NEED HERE #\n",
    "        # ############################################# #\n",
    "        self.gamma = gamma\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "        self.policy_learning_rate = policy_learning_rate\n",
    "        self.tau = tau\n",
    "\n",
    "\n",
    "\n",
    "        # ################################################### #\n",
    "        # DEFINE A GAUSSIAN THAT WILL BE USED FOR EXPLORATION #\n",
    "        # ################################################### #\n",
    "        mean = torch.zeros(ACTION_SIZE)\n",
    "        std = 0.1 * torch.ones(ACTION_SIZE)\n",
    "        self.noise = DiagGaussian(mean, std)\n",
    "\n",
    "        # ############################### #\n",
    "        # WRITE ANY AGENT PARAMETERS HERE #\n",
    "        # ############################### #\n",
    "        self.counter=0\n",
    "        self.saveables.update(\n",
    "            {\n",
    "                \"actor\": self.actor,\n",
    "                \"actor_target\": self.actor_target,\n",
    "                \"critic\": self.critic,\n",
    "                \"critic_target\": self.critic_target,\n",
    "                \"policy_optim\": self.policy_optim,\n",
    "                \"critic_optim\": self.critic_optim,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "    def save(self, path: str, suffix: str = \"\") -> str:\n",
    "        \"\"\"Saves saveable PyTorch models under given path\n",
    "\n",
    "        The models will be saved in directory found under given path in file \"models_{suffix}.pt\"\n",
    "        where suffix is given by the optional parameter (by default empty string \"\")\n",
    "\n",
    "        :param path (str): path to directory where to save models\n",
    "        :param suffix (str, optional): suffix given to models file\n",
    "        :return (str): path to file of saved models file\n",
    "        \"\"\"\n",
    "        torch.save(self.saveables, path)\n",
    "        return path\n",
    "\n",
    "\n",
    "    def restore(self, filename: str, dir_path: str = None):\n",
    "        \"\"\"Restores PyTorch models from models file given by path\n",
    "\n",
    "        :param filename (str): filename containing saved models\n",
    "        :param dir_path (str, optional): path to directory where models file is located\n",
    "        \"\"\"\n",
    "\n",
    "        if dir_path is None:\n",
    "            dir_path, _ = os.path.split(os.path.abspath(__file__))\n",
    "        save_path = os.path.join(dir_path, filename)\n",
    "        checkpoint = torch.load(save_path)\n",
    "        for k, v in self.saveables.items():\n",
    "            v.load_state_dict(checkpoint[k].state_dict())\n",
    "\n",
    "\n",
    "    def schedule_hyperparameters(self, timestep: int, max_timesteps: int):\n",
    "        \"\"\"Updates the hyperparameters\n",
    "\n",
    "        **YOU MAY IMPLEMENT THIS FUNCTION FOR Q5**\n",
    "\n",
    "        This function is called before every episode and allows you to schedule your\n",
    "        hyperparameters.\n",
    "\n",
    "        :param timestep (int): current timestep at the beginning of the episode\n",
    "        :param max_timestep (int): maximum timesteps that the training loop will run for\n",
    "        \"\"\"\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "        #print(self.epsilon)\n",
    "        pass\n",
    "        #self.epsilon= self.epsilon_start+min(timestep, max_timesteps*self.exploration_fraction)*(self.epsilon_min-self.epsilon_start)/(max_timesteps*self.exploration_fraction)\n",
    "        #mean = torch.zeros(self.ACTION_SIZE)\n",
    "        ##std = self.epsilon * torch.ones(self.ACTION_SIZE)\n",
    "        #self.noise = DiagGaussian(mean, std)\n",
    "\n",
    "    def act(self, obs: np.ndarray, explore: bool):\n",
    "        \"\"\"Returns an action (should be called at every timestep)\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q4**\n",
    "\n",
    "        When explore is False you should select the best action possible (greedy). However, during exploration,\n",
    "        you should be implementing exporation using the self.noise variable that you should have declared in the __init__.\n",
    "        Use schedule_hyperparameters() for any hyperparameters that you want to change over time.\n",
    "\n",
    "        :param obs (np.ndarray): observation vector from the environment\n",
    "        :param explore (bool): flag indicating whether we should explore\n",
    "        :return (sample from self.action_space): action the agent should perform\n",
    "        \"\"\"\n",
    "        ### I PUT MY CODE HERE - beginning ###\n",
    "        #print(123)\n",
    "        \n",
    "        if explore:\n",
    "            return (self.actor(torch.tensor(obs))+self.noise.sample()).detach().numpy()\n",
    "        return self.actor(torch.tensor(obs)).detach().numpy()\n",
    "        #raise NotImplementedError(\"Needed for Q4\")\n",
    "        ### I PUT MY CODE HERE - end ###\n",
    "    def update(self, batch: Transition) -> Dict[str, float]:\n",
    "        #critic network update\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_target_input=torch.cat((batch[2],self.actor_target(batch[2])),1)\n",
    "        y_batch=batch[3]+self.gamma*(1-batch[4])*self.critic_target(critic_target_input)\n",
    "        critic_input=torch.cat((batch[0], batch[1]),1)\n",
    "        x_batch=self.critic(critic_input)\n",
    "        loss=torch.nn.MSELoss()\n",
    "        loss=loss(y_batch,x_batch)\n",
    "        loss.backward()\n",
    "        q_loss=float(loss.detach().numpy())\n",
    "        self.critic_optim.step()\n",
    "        for parameter in self.critic.parameters():\n",
    "            parameter.requires_grad=False\n",
    "\n",
    "        #actor network update\n",
    "        self.policy_optim.zero_grad()\n",
    "        critic_input=torch.cat((batch[0], self.actor(batch[0])),1)\n",
    "        critic_batch=self.critic(critic_input)\n",
    "        loss=-torch.mean(critic_batch)\n",
    "        p_loss=float(loss.detach().numpy())\n",
    "        loss.backward()\n",
    "        self.policy_optim.step()\n",
    "        for parameter in self.critic.parameters():\n",
    "            parameter.requires_grad=True\n",
    "        self.actor_target.soft_update(self.actor, tau=self.tau)\n",
    "        self.critic_target.soft_update(self\n",
    "                                       .critic, tau=self.tau)\n",
    "        return {\"p_loss\":p_loss, \"q_loss\":q_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "773fa29c-4e45-446a-82a9-f224d72b0006",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▍                                                         | 10011/400000 [00:50<30:26, 213.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at timestep 10011 returned a mean returns of -2.0219146147642695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████████▌                                           | 100850/400000 [08:59<25:09, 198.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at timestep 100850 returned a mean returns of -2.8383429143561143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400118it [34:45, 191.86it/s]                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation at timestep 400118 returned a mean returns of -2.522776103474226\n",
      "Saving to:  bipedal_q4_latest.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "licznik=0\n",
    "RENDER = True # FALSE FOR FASTER TRAINING / TRUE TO VISUALIZE ENVIRONMENT DURING EVALUATION\n",
    "SWEEP = False # TRUE TO SWEEP OVER POSSIBLE HYPERPARAMETER CONFIGURATIONS\n",
    "NUM_SEEDS_SWEEP = 10 # NUMBER OF SEEDS TO USE FOR EACH HYPERPARAMETER CONFIGURATION\n",
    "SWEEP_SAVE_RESULTS = True # TRUE TO SAVE SWEEP RESULTS TO A FILE\n",
    "SWEEP_SAVE_ALL_WEIGTHS = True # TRUE TO SAVE ALL WEIGHTS FROM EACH SEED\n",
    "ENV = \"BIPEDAL\" #\"PENDULUM\" OR \"BIPEDAL\"\n",
    "\n",
    "PENDULUM_CONFIG = {\n",
    "    \"eval_freq\": 2000,\n",
    "    \"eval_episodes\": 3,\n",
    "    \"policy_learning_rate\": 1e-3,\n",
    "    \"critic_learning_rate\": 1e-3,\n",
    "    \"critic_hidden_size\": [64, 64],\n",
    "    \"policy_hidden_size\": [64, 64],\n",
    "    \"tau\": 0.01,\n",
    "    \"batch_size\": 64,\n",
    "    \"buffer_capacity\": int(1e6),\n",
    "}\n",
    "PENDULUM_CONFIG.update(PENDULUM_CONSTANTS)\n",
    "\n",
    "BIPEDAL_CONFIG = {\n",
    "    \"eval_freq\": 2000,\n",
    "    \"critic_hidden_size\": [256, 236],\n",
    "    \"policy_hidden_size\": [128, 128],\n",
    "}\n",
    "BIPEDAL_CONFIG.update(BIPEDAL_CONSTANTS)\n",
    "\n",
    "### INCLUDE YOUR CHOICE OF HYPERPARAMETERS HERE ###\n",
    "BIPEDAL_HPARAMS = {\n",
    "    \"critic_hidden_size\": [[256,236]],\n",
    "    \"policy_hidden_size\": [[128,128]]\n",
    "    }\n",
    "\n",
    "SWEEP_RESULTS_FILE_BIPEDAL = \"DDPG-Bipedal-sweep-results-ex4.pkl\"\n",
    "\n",
    "\n",
    "def play_episode(\n",
    "        env,\n",
    "        agent,\n",
    "        replay_buffer,\n",
    "        train=True,\n",
    "        explore=True,\n",
    "        render=False,\n",
    "        max_steps=200,\n",
    "        batch_size=64,\n",
    "):\n",
    "\n",
    "    ep_data = defaultdict(list)\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    if render:\n",
    "        env.render()\n",
    "\n",
    "    episode_timesteps = 0\n",
    "    episode_return = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(obs, explore=explore)\n",
    "        nobs, reward, done, _ = env.step(action)\n",
    "        if train:\n",
    "            replay_buffer.push(\n",
    "                np.array(obs, dtype=np.float32),\n",
    "                np.array(action, dtype=np.float32),\n",
    "                np.array(nobs, dtype=np.float32),\n",
    "                np.array([reward], dtype=np.float32),\n",
    "                np.array([done], dtype=np.float32),\n",
    "            )\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                new_data = agent.update(batch)\n",
    "                for k, v in new_data.items():\n",
    "                    ep_data[k].append(v)\n",
    "\n",
    "        episode_timesteps += 1\n",
    "        episode_return += reward\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        if max_steps == episode_timesteps:\n",
    "            break\n",
    "        obs = nobs\n",
    "\n",
    "    return episode_timesteps, episode_return, ep_data\n",
    "\n",
    "\n",
    "def train(env: gym.Env, config, output: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Execute training of DDPG on given environment using the provided configuration\n",
    "\n",
    "    :param env (gym.Env): environment to train on\n",
    "    :param config: configuration dictionary mapping configuration keys to values\n",
    "    :param output (bool): flag whether evaluation results should be printed\n",
    "    :return (Tuple[List[float], List[float]]): eval returns during training, times of evaluation\n",
    "    \"\"\"\n",
    "    timesteps_elapsed = 0\n",
    "\n",
    "    agent = DDPG(\n",
    "        action_space=env.action_space, observation_space=env.observation_space, **config\n",
    "    )\n",
    "    replay_buffer = ReplayBuffer(config[\"buffer_capacity\"])\n",
    "\n",
    "    eval_returns_all = []\n",
    "    eval_timesteps_all = []\n",
    "    eval_times_all = []\n",
    "    run_data = defaultdict(list)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with tqdm(total=config[\"max_timesteps\"]) as pbar:\n",
    "        while timesteps_elapsed < config[\"max_timesteps\"]:\n",
    "            elapsed_seconds = time.time() - start_time\n",
    "            if elapsed_seconds > config[\"max_time\"]:\n",
    "                pbar.write(f\"Training ended after {elapsed_seconds}s.\")\n",
    "                break\n",
    "\n",
    "            agent.schedule_hyperparameters(timesteps_elapsed, config[\"max_timesteps\"])\n",
    "            episode_timesteps, ep_return, ep_data = play_episode(\n",
    "                env,\n",
    "                agent,\n",
    "                replay_buffer,\n",
    "                train=True,\n",
    "                explore=True,\n",
    "                render=False,\n",
    "                max_steps=config[\"episode_length\"],\n",
    "                batch_size=config[\"batch_size\"],\n",
    "            )\n",
    "            timesteps_elapsed += episode_timesteps\n",
    "            pbar.update(episode_timesteps)\n",
    "            for k, v in ep_data.items():\n",
    "                run_data[k].extend(v)\n",
    "            run_data[\"train_ep_returns\"].append(ep_return)\n",
    "\n",
    "            if timesteps_elapsed % config[\"eval_freq\"] < episode_timesteps:\n",
    "                eval_returns = 0\n",
    "                for _ in range(config[\"eval_episodes\"]):\n",
    "                    _, episode_return, _ = play_episode(\n",
    "                        env,\n",
    "                        agent,\n",
    "                        replay_buffer,\n",
    "                        train=False,\n",
    "                        explore=False,\n",
    "                        render=RENDER,\n",
    "                        max_steps=config[\"episode_length\"],\n",
    "                        batch_size=config[\"batch_size\"],\n",
    "                    )\n",
    "                    eval_returns += episode_return / config[\"eval_episodes\"]\n",
    "                if output:\n",
    "                    pbar.write(\n",
    "                        f\"Evaluation at timestep {timesteps_elapsed} returned a mean returns of {eval_returns}\"\n",
    "                    )\n",
    "                    # pbar.write(f\"Epsilon = {agent.epsilon}\")\n",
    "                eval_returns_all.append(eval_returns)\n",
    "                eval_timesteps_all.append(timesteps_elapsed)\n",
    "                eval_times_all.append(time.time() - start_time)\n",
    "                if eval_returns >= config[\"target_return\"]:\n",
    "                    pbar.write(\n",
    "                        f\"Reached return {eval_returns} >= target return of {config['target_return']}\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "    if config[\"save_filename\"]:\n",
    "        print(\"Saving to: \", agent.save(config[\"save_filename\"]))\n",
    "\n",
    "    return np.array(eval_returns_all), np.array(eval_timesteps_all), np.array(eval_times_all), run_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if ENV == \"PENDULUM\":\n",
    "    CONFIG = PENDULUM_CONFIG\n",
    "    HPARAMS_SWEEP = None # Not required for assignment\n",
    "    SWEEP_RESULTS_FILE = None # Not required for assignment\n",
    "elif ENV == \"BIPEDAL\":\n",
    "    CONFIG = BIPEDAL_CONFIG\n",
    "    HPARAMS_SWEEP = BIPEDAL_HPARAMS\n",
    "    SWEEP_RESULTS_FILE = SWEEP_RESULTS_FILE_BIPEDAL\n",
    "else:\n",
    "    raise(ValueError(f\"Unknown environment {ENV}\"))\n",
    "\n",
    "env = gym.make(CONFIG[\"env\"])\n",
    "\n",
    "if SWEEP and HPARAMS_SWEEP is not None:\n",
    "    config_list, swept_params = generate_hparam_configs(CONFIG, HPARAMS_SWEEP)\n",
    "    results = []\n",
    "    for config in config_list:\n",
    "        run = Run(config)\n",
    "        hparams_values = '_'.join([':'.join([key, str(config[key])]) for key in swept_params])\n",
    "        run.run_name = hparams_values\n",
    "        print(f\"\\nStarting new run...\")\n",
    "        for i in range(NUM_SEEDS_SWEEP):\n",
    "            print(f\"\\nTraining iteration: {i+1}/{NUM_SEEDS_SWEEP}\")\n",
    "            run_save_filename = '--'.join([run.config[\"algo\"], run.config[\"env\"], hparams_values, str(i)])\n",
    "            if SWEEP_SAVE_ALL_WEIGTHS:\n",
    "                run.set_save_filename(run_save_filename)\n",
    "            eval_returns, eval_timesteps, times, run_data = train(env, run.config, output=False)\n",
    "            run.update(eval_returns, eval_timesteps, times, run_data)\n",
    "        results.append(copy.deepcopy(run))\n",
    "        print(f\"Finished run with hyperparameters {hparams_values}. \"\n",
    "              f\"Mean final score: {run.final_return_mean} +- {run.final_return_ste}\")\n",
    "\n",
    "    if SWEEP_SAVE_RESULTS:\n",
    "        print(f\"Saving results to {SWEEP_RESULTS_FILE}\")\n",
    "        with open(SWEEP_RESULTS_FILE, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "else:\n",
    "    _ = train(env, CONFIG)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04a6c7e9-a3b4-4d88-922d-f9801b07731a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      2\u001b[0m     _, episode_return, _ \u001b[38;5;241m=\u001b[39m play_episode(\n\u001b[1;32m      3\u001b[0m         env,\n\u001b[0;32m----> 4\u001b[0m         \u001b[43magent\u001b[49m,\n\u001b[1;32m      5\u001b[0m         replay_buffer,\n\u001b[1;32m      6\u001b[0m         train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m         explore\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m         render\u001b[38;5;241m=\u001b[39mRENDER,\n\u001b[1;32m      9\u001b[0m         max_steps\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_length\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    _, episode_return, _ = play_episode(\n",
    "        env,\n",
    "        agent,\n",
    "        replay_buffer,\n",
    "        train=False,\n",
    "        explore=False,\n",
    "        render=RENDER,\n",
    "        max_steps=config[\"episode_length\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b96a6e-c58c-4db9-82f5-5e8b04b0fa8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
